{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/te-supreme-beef/fresho/blob/main/fresho_assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Three Step Approach\n",
        "\n",
        "### 1. Biencoder\n",
        "  * Format extracted strings\n",
        "  * Generate an embedding for all unique product_description = product_name, quantity type name pairs from supplier df\n",
        "  * Generate an embedding for extracted product_descriptions from predictions_df\n",
        "  * Filter to top 50 product_decriptions from supplier_df for each predictions_df extracted product description.\n",
        "    * Make sure to suggested_proudct_description if not included.\n",
        "  * Accuracy of teop 50 recs puts upper limit on cross encoder performance.\n",
        "\n",
        "### 2. Cross Encoder and rerank\n",
        "  * Train cross encoder model for more detailed embedding similarities.\n",
        "  * Evaluation metric f1.\n",
        "  * Use trained cross encoder to evaluate extracted_product_description from predictions_df to get product_id, quantity_type_id and product_name\n",
        "\n",
        "### 3. Quantity - didn't get to this\n",
        "\n",
        "* Regression/Classification: This can be a regression problem on the value of extracted_quantity, or a multi-class classification if quantities are limited (e.g., 1, 2, 5, 10)."
      ],
      "metadata": {
        "id": "N_xeSFART3xj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZaESVa2SDOo"
      },
      "outputs": [],
      "source": [
        "!pip install pandasql\n",
        "!pip install sentence-transformers\n",
        "!pip install datasets accelerate evaluate scikit-learn\n",
        "!pip install transformers --upgrade\n",
        "!pip install pandasql"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "476xQEqbSIXm"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYBZPil3SHoO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "from pandasql import sqldf\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mount Google Drive"
      ],
      "metadata": {
        "id": "fPTI_VrTrENS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bde5bb55"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNYH-AnLPcAd"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e4f8f8c"
      },
      "outputs": [],
      "source": [
        "drive_path = '/content/drive/My Drive/'\n",
        "directory_name = 'fresho/fresho_take_home'\n",
        "file_name = 'predictions.jsonl'\n",
        "\n",
        "full_path = os.path.join(drive_path, directory_name, file_name)\n",
        "\n",
        "data = []\n",
        "if os.path.exists(full_path):\n",
        "    with open(full_path, 'r') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    print(f\"Successfully loaded {len(data)} lines from {full_path}.\")\n",
        "    # Display the first few entries to verify\n",
        "    if data:\n",
        "        print(\"First 5 entries:\")\n",
        "        for item in data[:5]:\n",
        "            print(item)\n",
        "    else:\n",
        "        print(\"The .jsonl file was empty.\")\n",
        "else:\n",
        "    print(f\"Error: File not found at '{full_path}'. Please check the file name and path.\")\n",
        "\n",
        "predictions_df = pd.DataFrame(data)\n",
        "display(predictions_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dy8gvkkPG_q"
      },
      "outputs": [],
      "source": [
        "\n",
        "file_name = 'supplier_inventory.jsonl'\n",
        "\n",
        "full_path = os.path.join(drive_path, directory_name, file_name)\n",
        "\n",
        "data = []\n",
        "if os.path.exists(full_path):\n",
        "    with open(full_path, 'r') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    print(f\"Successfully loaded {len(data)} lines from {full_path}.\")\n",
        "    if data:\n",
        "        print(\"First 5 entries:\")\n",
        "        for item in data[:5]:\n",
        "            print(item)\n",
        "    else:\n",
        "        print(\"The .jsonl file was empty.\")\n",
        "else:\n",
        "    print(f\"Error: File not found at '{full_path}'. Please check the file name and path.\")\n",
        "\n",
        "supplier_df = pd.DataFrame(data)\n",
        "display(supplier_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1fyY2nfl7oB"
      },
      "source": [
        "# Product Name Standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51a58378"
      },
      "outputs": [],
      "source": [
        "def standardize_name(name):\n",
        "    if isinstance(name, str):\n",
        "        # Convert to lowercase\n",
        "        name = name.lower()\n",
        "        # 1. Standardize 'x - y' to 'x to y' or similar.\n",
        "        name = re.sub(r'(\\d+)\\s*-\\s*(\\d+)', r'\\1 to \\2', name, flags=re.IGNORECASE)\n",
        "        # It might be safer to just ensure there is a space around the hyphen if it's numerical:\n",
        "        name = re.sub(r'(\\d)([/-])(\\d)', r'\\1 \\2 \\3', name)\n",
        "        #Removes commas, quotes, periods, etc., that don't change the meaning (e.g., \"Stew Pack\" vs \"Stew Pack.\").\n",
        "        name = re.sub(r'[^\\w\\s]', '', name)\n",
        "        # Replace slashes with spaces\n",
        "        name = re.sub(r'[\\\\/]', ' ', name)\n",
        "        # Replaces multiple spaces, newlines, or tabs with a single space (e.g., \"Stew\\nPack\" $\\rightarrow$ \"Stew Pack\").\n",
        "        name = re.sub(r'\\s+', ' ', name).strip()\n",
        "    return name\n",
        "\n",
        "supplier_df['standardized_product_name'] = supplier_df['product_name'].apply(standardize_name)\n",
        "\n",
        "predictions_df['standardized_extracted_product_name'] = predictions_df['extracted_product_name'].apply(standardize_name)\n",
        "predictions_df['standardized_confirmed_product_name'] = predictions_df['confirmed_product_name'].apply(standardize_name)\n",
        "predictions_df['standardized_suggested_product_name'] = predictions_df['suggested_product_name'].apply(standardize_name)\n",
        "\n",
        "display(predictions_df[['extracted_product_name', 'standardized_extracted_product_name', 'confirmed_product_name', 'standardized_confirmed_product_name', 'suggested_product_name', 'standardized_suggested_product_name']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantity Type Name"
      ],
      "metadata": {
        "id": "sqijw2EOJtpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate quantity types for standardization rules\n",
        "\n",
        "query = '''\n",
        "select * from (\n",
        "select\n",
        "extracted_quantity_type_name, count(*) as total_types\n",
        "from predictions_df\n",
        "group by 1\n",
        "order by count(*) desc\n",
        ")iq where total_types > 50\n",
        "'''\n",
        "\n",
        "# Execute the SQL query\n",
        "sql_result = sqldf(query)\n",
        "\n",
        "# Display the results\n",
        "display(sql_result)\n",
        "\n",
        "# drive_path = '/content/drive/My Drive/'\n",
        "# directory_name = 'fresho/fresho_take_home/quantity_type_names.csv'\n",
        "\n",
        "# quantity_type_full_path = os.path.join(drive_path, directory_name)\n",
        "\n",
        "# sql_result.to_parquet(quantity_type_full_path, index=False)\n"
      ],
      "metadata": {
        "id": "VcsagIpMJtQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Dictionary for fixing specific OCR errors found in your data file\n",
        "TYPO_CORRECTION = {\n",
        "    'unch': 'bunch',\n",
        "    'bncet': 'bunch',\n",
        "    'eteh': 'each',\n",
        "    'unie': 'unit',\n",
        "    'counei': 'count',\n",
        "    'coune': 'count',\n",
        "    'anett': 'punnet',\n",
        "    'acke': 'pack',\n",
        "    'pac': 'pack',\n",
        "    'ba': 'bag',\n",
        "    'bot': 'bottle',\n",
        "    'bottl': 'bottle',\n",
        "    'kge': 'kg',\n",
        "    'ilogram': 'kg',\n",
        "    'kilogram': 'kg',\n",
        "    'gram': 'g',\n",
        "    'litr': 'l',\n",
        "    'ltr': 'l',\n",
        "    'mlt': 'ml',\n",
        "    'crt': 'carton',\n",
        "    'ctn': 'carton',\n",
        "    'bx': 'box',\n",
        "    'caez': 'case',\n",
        "    'ase': 'case',\n",
        "    'bdl': 'bundle',\n",
        "    'doz': 'dozen',\n",
        "    'ea': 'each',\n",
        "    'pre-ao': 'pack',\n",
        "    'unknow': 'unit',\n",
        "}\n",
        "\n",
        "# Conversion factors: All weights/volumes standardized to base units (kilograms/liters)\n",
        "UNIT_CONVERSIONS = {\n",
        "    # Weight (Standardized to Kilograms)\n",
        "    'kg': 1.0,\n",
        "    'kilo': 1.0,\n",
        "    'g': 0.001,\n",
        "    'gram': 0.001,\n",
        "    'lb': 0.453592,\n",
        "    'oz': 0.0283495,\n",
        "\n",
        "    # Volume (Standardized to Liters)\n",
        "    'l': 1.0,\n",
        "    'liter': 1.0,\n",
        "    'ml': 0.001,\n",
        "    'milliliter': 0.001,\n",
        "    'fl oz': 0.0295735,\n",
        "    'qt': 0.946353,\n",
        "\n",
        "    # Count (Standardized to Count 'ct')\n",
        "    'ct': 1.0,\n",
        "    'count': 1.0,\n",
        "    'dozen': 12.0,\n",
        "    'each': 1.0,\n",
        "    'bunch': 1.0,\n",
        "    'unit': 1.0,\n",
        "    'case': 1.0,\n",
        "    'box': 1.0,\n",
        "    'tub': 1.0,\n",
        "    'punnet': 1.0,\n",
        "    'carton': 1.0,\n",
        "    'pcs': 1.0,\n",
        "    'piece': 1.0,\n",
        "    'pc': 1.0,\n",
        "}\n",
        "\n",
        "# Mapping of messy packaging terms to canonical categories\n",
        "PACKAGING_ABSTRACTIONS = {\n",
        "    # Flexible/Soft Containers\n",
        "    'bag': 'container_flexible',\n",
        "    'pouch': 'container_flexible',\n",
        "    'package': 'container_flexible',\n",
        "    'wrapper': 'container_flexible',\n",
        "    'sleeve': 'container_flexible',\n",
        "\n",
        "    # Rigid/Hard Containers\n",
        "    'box': 'container_rigid',\n",
        "    'carton': 'container_rigid',\n",
        "    'case': 'container_rigid',\n",
        "    'clamshell': 'container_rigid',\n",
        "    'tray': 'container_rigid',\n",
        "    'punnet': 'container_rigid',\n",
        "    'tub': 'container_rigid',\n",
        "\n",
        "    # Liquid Containers\n",
        "    'bottle': 'container_liquid',\n",
        "    'jug': 'container_liquid',\n",
        "    'can': 'container_liquid',\n",
        "    'jar': 'container_liquid',\n",
        "\n",
        "    # Discrete Units\n",
        "    'unit': 'single_unit',\n",
        "    'each': 'single_unit',\n",
        "    'pc': 'single_unit',\n",
        "    'piece': 'single_unit',\n",
        "\n",
        "    # Placeholder for non-quantifiable items\n",
        "    'variable': 'variable_weight',\n",
        "    'bulk': 'variable_weight',\n",
        "}\n",
        "\n",
        "\n",
        "def clean_special_chars(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Step 0 (Pre-processing): Removes noise characters and handles specific formatting.\n",
        "    Refactored to aggressively replace non-alphanumeric, non-space, non-decimal\n",
        "    characters with spaces first.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    # 1. Replace 'x' with space if used as a separator (e.g., 1x1ea -> 1 1ea)\n",
        "    if 'box' not in text:\n",
        "      text = text.replace('x', ' ')\n",
        "\n",
        "    # 2. Replace all non-alphanumeric/non-decimal characters with space\n",
        "    # This aggressively handles: ( ) , - / % ~ ! etc.\n",
        "    text = re.sub(r'[^a-z0-9\\s\\.]', ' ', text)\n",
        "\n",
        "    # 3. Insert space between Numbers (incl. decimals) and Letters (e.g., \"10kg\" -> \"10 kg\")\n",
        "    # Using a non-capturing group for the decimal part for clean separation\n",
        "    text = re.sub(r'(\\d+(?:\\.\\d+)?)([a-z]+)', r'\\1 \\2', text)\n",
        "\n",
        "    # 4. Insert space between Letters and Numbers (e.g., \"No1\" -> \"No 1\")\n",
        "    text = re.sub(r'([a-z]+)(\\d+)', r'\\1 \\2', text)\n",
        "\n",
        "    # 5. Collapse multiple spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def apply_typo_correction(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Step 0.5 (Correction): Maps individual tokens using the typo dictionary.\n",
        "\n",
        "    FIXED: The logic for skipping single characters now correctly excludes\n",
        "    numbers and only targets single letters that are not standard unit abbreviations.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    corrected_words = []\n",
        "\n",
        "    standard_single_letter_units = ['g', 'l', 'm', 'k']\n",
        "\n",
        "    for word in words:\n",
        "        if word in TYPO_CORRECTION:\n",
        "            corrected_words.append(TYPO_CORRECTION[word])\n",
        "\n",
        "        # Skip single *letters* that aren't standard unit abbreviations (OCR noise)\n",
        "        # This now correctly keeps single-digit numbers like '2' or '5'.\n",
        "        elif len(word) == 1 and word.isalpha() and word not in standard_single_letter_units:\n",
        "            continue\n",
        "\n",
        "        else:\n",
        "            corrected_words.append(word)\n",
        "\n",
        "    return \" \".join(corrected_words)\n",
        "\n",
        "\n",
        "def perform_unit_conversion(value, raw_unit):\n",
        "    \"\"\"Helper to convert value/unit pair if unit is valid.\"\"\"\n",
        "    # SORT KEYS BY LENGTH DESCENDING: matches 'fl oz' before 'oz'\n",
        "    sorted_keys = sorted(UNIT_CONVERSIONS.keys(), key=len, reverse=True)\n",
        "\n",
        "    # Find the longest matching unit key\n",
        "    unit_key = next((key for key in sorted_keys if raw_unit.startswith(key)), None)\n",
        "\n",
        "    if unit_key:\n",
        "        conversion_factor = UNIT_CONVERSIONS[unit_key]\n",
        "        canonical_value = value * conversion_factor\n",
        "\n",
        "        # Determine the standardized unit display name\n",
        "        if unit_key in ['kg', 'g', 'lb', 'oz', 'kilo', 'gram']:\n",
        "            unit_display = 'kg'\n",
        "        elif unit_key in ['l', 'ml', 'fl oz', 'qt', 'liter', 'milliliter']:\n",
        "            unit_display = 'L'\n",
        "        elif unit_key in ['ct', 'count', 'dozen', 'each', 'bunch', 'unit', 'case', 'box', 'punnet', 'pcs', 'pc']:\n",
        "            unit_display = 'ct'\n",
        "        else:\n",
        "            unit_display = unit_key\n",
        "\n",
        "        return f\"{canonical_value:.3f} {unit_display}\"\n",
        "    return None\n",
        "\n",
        "def canonicalize_units(quantity_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Step 1: Parses cleaned quantity text, converts to canonical units (kg, L, ct).\n",
        "    Prioritizes specific units (Weight/Volume) over generic counts.\n",
        "    \"\"\"\n",
        "\n",
        "    # Collect ALL matches of the \"Number Unit\" pattern (e.g. \"1 each\", \"2 kg\")\n",
        "    # Regex updated to robustly capture decimals: (\\d+(?:\\.\\d+)?)\n",
        "    candidates = []\n",
        "    for match in re.finditer(r'(\\d+(?:\\.\\d+)?)\\s?([a-zA-Z\\s\\.]+)', quantity_text):\n",
        "        value = float(match.group(1))\n",
        "        raw_unit = match.group(2).lower().strip()\n",
        "        result = perform_unit_conversion(value, raw_unit)\n",
        "        if result:\n",
        "            candidates.append(result)\n",
        "\n",
        "    # Filter candidates to prioritize Weight (kg) or Volume (L)\n",
        "    priority_candidates = [res for res in candidates if res.endswith(' kg') or res.endswith(' L')]\n",
        "\n",
        "    # If we found a priority match (e.g., \"1.000 kg\"), return it immediately\n",
        "    if priority_candidates:\n",
        "        return priority_candidates[0]\n",
        "\n",
        "    # Check Strategy 2: \"Unit Number\" pattern (e.g. \"Box 40\") - Less common, but useful\n",
        "    match_reverse = re.search(r'([a-zA-Z\\s\\.]+)\\s?(\\d+(?:\\.\\d+)?)', quantity_text, re.IGNORECASE)\n",
        "    if match_reverse:\n",
        "        raw_unit = match_reverse.group(1).lower().strip()\n",
        "        value_str = match_reverse.group(2)\n",
        "        if value_str:\n",
        "            value = float(value_str)\n",
        "            result = perform_unit_conversion(value, raw_unit)\n",
        "            if result:\n",
        "                return result\n",
        "\n",
        "    # Fallback: if we found any generic candidates in Strategy 1 (e.g. \"1 ct\"), return the first one\n",
        "    if candidates:\n",
        "        return candidates[0]\n",
        "\n",
        "    return quantity_text.lower().strip()\n",
        "\n",
        "\n",
        "def normalize_format(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Step 2: Basic normalization (lowercasing, punctuation removal).\n",
        "    This function is now mostly redundant but kept for flow.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    # Note: Using the same aggressive special char removal as in clean_special_chars\n",
        "    # But only removing punctuation, not splitting numbers/letters again\n",
        "    text = re.sub(r'[^a-z0-9\\s\\.]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def abstract_packaging(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Step 3: Maps specific packaging terms to a broader category.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "\n",
        "    for word in words:\n",
        "        if word in PACKAGING_ABSTRACTIONS:\n",
        "            return PACKAGING_ABSTRACTIONS[word]\n",
        "\n",
        "    if any(key in text for key in ['variable', 'bulk', 'loose']):\n",
        "        return 'variable_weight'\n",
        "\n",
        "    return 'unspecified_packaging'\n",
        "\n",
        "\n",
        "def standardize_quantity_type(raw_quantity_type: str) -> str:\n",
        "    \"\"\"\n",
        "    MAIN FUNCTION: Combines cleaning, correction, and standardization steps.\n",
        "    Returns the final standardized string.\n",
        "    \"\"\"\n",
        "    if not raw_quantity_type or pd.isna(raw_quantity_type):\n",
        "        return \"unspecified_quantity\"\n",
        "\n",
        "    # 1. Clean noise (\"Each (1KG)\" -> \"each 1 kg\")\n",
        "    cleaned_text = clean_special_chars(str(raw_quantity_type))\n",
        "\n",
        "    # 2. Fix OCR typos\n",
        "    typo_fixed_text = apply_typo_correction(cleaned_text)\n",
        "\n",
        "    # 3. Canonicalize units (Extracts measurement: \"each 1 kg\" -> \"1.000 kg\")\n",
        "    canonical_unit_str = canonicalize_units(typo_fixed_text)\n",
        "\n",
        "    # 4. Normalize format (for packaging abstraction)\n",
        "    normalized_text = normalize_format(typo_fixed_text)\n",
        "\n",
        "    # 5. Abstract packaging (\"each 1 kg\" -> \"single_unit\")\n",
        "    packaging_category = abstract_packaging(normalized_text)\n",
        "\n",
        "    # Check if a numerical unit was successfully extracted\n",
        "    # If the string is a canonical unit (e.g., \"1.000 kg\"), use it with the packaging category.\n",
        "    if re.search(r'^\\d+\\.\\d+\\s(kg|L|ct)$', canonical_unit_str):\n",
        "        return f\"{canonical_unit_str} in {packaging_category}\"\n",
        "    else:\n",
        "        # Fallback to abstract category or clean text\n",
        "        if packaging_category != 'unspecified_packaging':\n",
        "            return packaging_category\n",
        "        return normalized_text\n",
        "\n",
        "    return typo_fixed_text"
      ],
      "metadata": {
        "id": "l2_b0_XFxy_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supplier_df['standardized_quantity_type_name'] = supplier_df['quantity_type_name'].apply(standardize_quantity_type)\n",
        "\n",
        "predictions_df['standardized_extracted_quantity_type_name'] = predictions_df['extracted_quantity_type_name'].apply(standardize_quantity_type)\n",
        "predictions_df['standardized_confirmed_quantity_type_name'] = predictions_df['confirmed_quantity_type_name'].apply(standardize_quantity_type)\n",
        "predictions_df['standardized_suggested_quantity_type_name'] = predictions_df['suggested_quantity_type_name'].apply(standardize_quantity_type)\n",
        "\n",
        "display(predictions_df[[\n",
        "  'extracted_quantity_type_name',\n",
        "  'standardized_extracted_quantity_type_name',\n",
        "  'confirmed_quantity_type_name',\n",
        "  'standardized_confirmed_quantity_type_name'\n",
        "]].head(20))"
      ],
      "metadata": {
        "id": "f3ii2tp42Kvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Create embeddings and generate topk products"
      ],
      "metadata": {
        "id": "1piT5X5kBrmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rich_text_separated(standardized_name: str, standardized_quantity_type: str) -> str:\n",
        "\n",
        "    SEPARATOR = \" | \"\n",
        "\n",
        "    name = standardized_name.strip()\n",
        "    quantity_type = standardized_quantity_type.strip()\n",
        "\n",
        "    return f\"{name}{SEPARATOR}{quantity_type}\"\n",
        "\n",
        "supplier_df['standardized_product_description'] = supplier_df.apply(\n",
        "    lambda row: create_rich_text_separated(\n",
        "        row['standardized_product_name'],\n",
        "        row['standardized_quantity_type_name']\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "predictions_df['standardized_extracted_product_description'] = predictions_df.apply(\n",
        "    lambda row: create_rich_text_separated(\n",
        "        row['standardized_extracted_product_name'],\n",
        "        row['standardized_extracted_quantity_type_name']\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "predictions_df['standardized_confirmed_product_description'] = predictions_df.apply(\n",
        "    lambda row: create_rich_text_separated(\n",
        "        row['standardized_confirmed_product_name'],\n",
        "        row['standardized_confirmed_quantity_type_name']\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "predictions_df['standardized_suggested_product_description'] = predictions_df.apply(\n",
        "    lambda row: create_rich_text_separated(\n",
        "        row['standardized_suggested_product_name'],\n",
        "        row['standardized_suggested_quantity_type_name']\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# display(supplier_df['standardized_product_description'].head(20))\n",
        "display(predictions_df[['standardized_extracted_product_description','standardized_confirmed_product_description','standardized_suggested_product_description']].head(20))"
      ],
      "metadata": {
        "id": "SsTTWjVuz0oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8db440b"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
        "\n",
        "supplier_name_unique_df = supplier_df[['standardized_product_description','product_id','quantity_type_id']].drop_duplicates().reset_index(drop=True)\n",
        "supplier_name_unique_df['product_desc_embeddings'] = [embedding for embedding in model.encode(supplier_name_unique_df['standardized_product_description'].tolist(), show_progress_bar=True)]\n",
        "display(supplier_name_unique_df.head())\n",
        "def get_embedding_for_text(text):\n",
        "\n",
        "    embedding = model.encode(text, convert_to_numpy=True)\n",
        "\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53dcd723"
      },
      "outputs": [],
      "source": [
        "predictions_df['extracted_product_desc_embeddings'] = [embedding for embedding in model.encode(predictions_df['standardized_extracted_product_description'].tolist(), show_progress_bar=True)]\n",
        "predictions_df['standardized_confirmed_product_desc_embeddings'] = [embedding for embedding in model.encode(predictions_df['standardized_confirmed_product_description'].tolist(), show_progress_bar=True)]\n",
        "predictions_df['standardized_suggested_product_desc_embeddings'] = [embedding for embedding in model.encode(predictions_df['standardized_suggested_product_description'].tolist(), show_progress_bar=True)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Catalogue Filtering\n",
        "\n",
        "* Goal get a solid hard negative for every extracted product name\n",
        "  * Use incorrect suggested product_name and quatity_name_type where is_prediction_correct = 0\n"
      ],
      "metadata": {
        "id": "jNbFcidmOyv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "supplier = torch.tensor(np.vstack(supplier_name_unique_df['product_desc_embeddings'].values), device=device)\n",
        "queries  = torch.tensor(np.vstack(predictions_df['extracted_product_desc_embeddings'].values), device=device)\n",
        "\n",
        "supplier = torch.nn.functional.normalize(supplier, dim=1)\n",
        "queries  = torch.nn.functional.normalize(queries, dim=1)\n",
        "\n",
        "K = 50\n",
        "batch_size = 1024\n",
        "\n",
        "all_topk_idx = []\n",
        "all_topk_scores = []\n",
        "\n",
        "for i in range(0, len(queries), batch_size):\n",
        "    q = queries[i:i+batch_size]\n",
        "\n",
        "    sim = q @ supplier.T\n",
        "\n",
        "    # top-K\n",
        "    scores, idx = torch.topk(sim, K, dim=1)\n",
        "    all_topk_idx.append(idx.cpu().numpy())\n",
        "    all_topk_scores.append(scores.cpu().numpy())\n",
        "\n",
        "topk_idx = np.vstack(all_topk_idx)\n",
        "topk_scores = np.vstack(all_topk_scores)\n"
      ],
      "metadata": {
        "id": "m_OHrByLVge_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supplier_reset = supplier_name_unique_df.reset_index(drop=True)\n",
        "\n",
        "# Convert supplier metadata columns to NumPy arrays\n",
        "product_descs = supplier_reset['standardized_product_description'].to_numpy()\n",
        "product_ids = supplier_reset['product_id'].to_numpy()\n",
        "product_quantity_ids = supplier_reset['quantity_type_id'].to_numpy()\n",
        "\n",
        "\n",
        "# Index using topk_idx (2D) â†’ still 2D, then convert to list of lists\n",
        "topk_product_desc_raw = product_descs[topk_idx].tolist()\n",
        "topk_product_ids_raw = product_ids[topk_idx].tolist()\n",
        "topk_product_quantity_ids_raw = product_quantity_ids[topk_idx].tolist()\n",
        "\n",
        "# For negatives\n",
        "predictions_df['topk_standardized_product_desc'] = [[] for _ in range(len(predictions_df))]\n",
        "# For reranking\n",
        "predictions_df['topk_product_ids'] = [[] for _ in range(len(predictions_df))]\n",
        "predictions_df['topk_qauntity_type_ids'] = [[] for _ in range(len(predictions_df))]\n",
        "\n",
        "predictions_df['topk_standardized_product_desc_unfiltered'] = [[] for _ in range(len(predictions_df))]\n",
        "predictions_df['topk_product_ids_unfiltered'] = [[] for _ in range(len(predictions_df))]\n",
        "predictions_df['topk_qauntity_type_ids_unfiltered'] = [[] for _ in range(len(predictions_df))]\n",
        "predictions_df['topk_standardized_product_desc_unfiltered_with_confirmed'] = [[] for _ in range(len(predictions_df))]\n",
        "predictions_df['topk_product_ids_unfiltered_with_confirmed'] = [[] for _ in range(len(predictions_df))]\n",
        "predictions_df['topk_qauntity_type_ids_unfiltered_with_confirmed'] = [[] for _ in range(len(predictions_df))]\n",
        "\n",
        "predictions_df['topk_similarity_score'] = [[] for _ in range(len(predictions_df))]\n",
        "predictions_df['confirmed_desc_in_topk_product_desc_raw'] = 0\n",
        "\n",
        "for i, row in predictions_df.iterrows():\n",
        "    confirmed_product_desc = row['standardized_confirmed_product_description']\n",
        "    confirmed_desc_in_topk_product_name_raw = 0\n",
        "    if confirmed_product_desc in topk_product_desc_raw[i]:\n",
        "      confirmed_desc_in_topk_product_name_raw = 1\n",
        "\n",
        "    filtered_product_desc = []\n",
        "    filtered_similarity_scores = []\n",
        "\n",
        "    current_topk_desc = topk_product_desc_raw[i]\n",
        "    current_topk_scores = topk_scores[i]\n",
        "    current_topk_product_ids = topk_product_ids_raw[i]\n",
        "    current_topk_product_quantity_type_ids = topk_product_quantity_ids_raw[i]\n",
        "\n",
        "    # suggested product name likely a really strong negative be sure to include\n",
        "    if row['is_prediction_correct'] == 0 and row['standardized_suggested_product_name'] not in current_topk_desc:\n",
        "        filtered_product_desc.append(row['standardized_suggested_product_name'])\n",
        "        filtered_similarity_scores.append(0)\n",
        "\n",
        "    for j in range(len(current_topk_desc)):\n",
        "        if topk_product_desc_raw[j] != confirmed_product_desc:\n",
        "            filtered_product_desc.append(current_topk_desc[j])\n",
        "            filtered_similarity_scores.append(current_topk_scores[j])\n",
        "\n",
        "    current_topk_desc_all =current_topk_desc.copy()\n",
        "    current_topk_product_ids_all =current_topk_product_ids.copy()\n",
        "    current_topk_product_quantity_type_ids_all =current_topk_product_quantity_type_ids.copy()\n",
        "\n",
        "    if confirmed_product_desc not in current_topk_desc_all:\n",
        "      current_topk_desc_all.append(confirmed_product_desc)\n",
        "      current_topk_product_ids_all.append(row['confirmed_product_id'])\n",
        "      current_topk_product_quantity_type_ids_all.append(row['confirmed_quantity_type_id'])\n",
        "\n",
        "\n",
        "    predictions_df.at[i, 'topk_standardized_product_desc'] = filtered_product_desc\n",
        "    predictions_df.at[i, 'topk_similarity_score'] = filtered_similarity_scores\n",
        "    predictions_df.at[i, 'topk_standardized_product_desc_unfiltered'] = current_topk_desc\n",
        "    predictions_df.at[i, 'topk_product_ids_unfiltered'] = current_topk_product_ids\n",
        "    predictions_df.at[i, 'topk_qauntity_type_ids_unfiltered'] = current_topk_product_quantity_type_ids\n",
        "    predictions_df.at[i, 'topk_standardized_product_desc_unfiltered_with_confirmed'] = current_topk_desc_all\n",
        "    predictions_df.at[i, 'topk_product_ids_unfiltered_with_confirmed'] = current_topk_product_ids_all\n",
        "    predictions_df.at[i, 'topk_qauntity_type_ids_unfiltered_with_confirmed'] = current_topk_product_quantity_type_ids_all\n",
        "\n",
        "    predictions_df.at[i, 'confirmed_name_in_topk_product_name_raw']= confirmed_desc_in_topk_product_name_raw\n",
        "\n",
        "display(predictions_df.head())"
      ],
      "metadata": {
        "id": "76E1cjjFMd6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bicoder_accuracy = (predictions_df['confirmed_name_in_topk_product_name_raw'] == 1).mean()\n",
        "print(f\"Bicoder Accuracy: {bicoder_accuracy}\")"
      ],
      "metadata": {
        "id": "4-XY6p-n95I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display(predictions_df[['standardized_extracted_product_description','standardized_confirmed_product_description','topk_standardized_product_desc_unfiltered']][predictions_df['confirmed_name_in_topk_product_name_raw'] == 0].head(10))\n"
      ],
      "metadata": {
        "id": "Cj7lLHk85HYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FIXME Updated embeddings generation with fine tuning"
      ],
      "metadata": {
        "id": "aNAH_epdBwI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Cross Encoder"
      ],
      "metadata": {
        "id": "2FpatDxqz-2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NEGATIVES_PER_POS = 5\n",
        "TEST_SIZE = 0.1\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "def format_request(name):\n",
        "    return f\"Product: {name}\"\n",
        "\n",
        "def format_candidate(name):\n",
        "    return f\"Product: {name}\"\n",
        "\n",
        "pairs = []\n",
        "labels = []\n",
        "\n",
        "for idx, row in tqdm(predictions_df.iterrows(), total=len(predictions_df), desc=\"Generating cross-encoder pairs\"):\n",
        "    query_name = row['standardized_extracted_product_description']\n",
        "    confirmed_name = row['standardized_confirmed_product_description']\n",
        "\n",
        "    # Add positive pair\n",
        "    pairs.append((format_request(query_name), format_candidate(confirmed_name)))\n",
        "    labels.append(1)\n",
        "\n",
        "    # topk_product_name already excludes the confirmed_product_id and includes hard negatives if prediction was incorrect\n",
        "    available_negatives = row['topk_standardized_product_desc']\n",
        "\n",
        "    # Ensure we don't accidentally pick the confirmed_name if it somehow reappeared (e.g., due to different standardization)\n",
        "    available_negatives = [n for n in available_negatives if n != confirmed_name]\n",
        "\n",
        "    # Sample unique negatives up to NEGATIVES_PER_POS\n",
        "    sampled_negatives = random.sample(available_negatives, min(NEGATIVES_PER_POS, len(available_negatives)))\n",
        "\n",
        "    for neg_name in sampled_negatives:\n",
        "        pairs.append((format_request(query_name), format_candidate(neg_name)))\n",
        "        labels.append(0)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_pairs = pd.DataFrame(pairs, columns=['query', 'candidate'])\n",
        "df_pairs['label'] = labels\n",
        "\n",
        "# Train/validation split\n",
        "train_df, val_df = train_test_split(df_pairs, test_size=TEST_SIZE, stratify=df_pairs['label'], random_state=SEED)\n",
        "\n",
        "print(f\"Generated {len(df_pairs)} total pairs.\")\n",
        "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}\")\n",
        "\n",
        "display(train_df.head())\n",
        "display(val_df.head())"
      ],
      "metadata": {
        "id": "4P7v2JHZlxkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def tokenize_batch(examples):\n",
        "    return tokenizer(\n",
        "        examples['query'],\n",
        "        examples['candidate'],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
        "val_ds = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
        "\n",
        "train_ds = train_ds.map(tokenize_batch, batched=True)\n",
        "val_ds = val_ds.map(tokenize_batch, batched=True)\n",
        "\n",
        "columns_to_keep = ['input_ids', 'attention_mask', 'label']\n",
        "train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in columns_to_keep])\n",
        "val_ds = val_ds.remove_columns([c for c in val_ds.column_names if c not in columns_to_keep])\n",
        "\n",
        "train_ds.set_format(type='torch', columns=columns_to_keep)\n",
        "val_ds.set_format(type='torch', columns=columns_to_keep)\n",
        "\n",
        "print(\"Train dataset prepared:\")\n",
        "print(train_ds)\n",
        "print(\"Validation dataset prepared:\")\n",
        "print(val_ds)\n"
      ],
      "metadata": {
        "id": "YOMdNm7tmLMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import wandb\n",
        "\n",
        "wandb.login(key=userdata.get('WANDB_API_KEY'))"
      ],
      "metadata": {
        "id": "tNC8YqwPMduf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import os\n",
        "\n",
        "drive_path = '/content/drive/My Drive/'\n",
        "directory_name = 'fresho/fresho_take_home/cross-encoder-best'\n",
        "checkpoint_name = 'fresho/fresho_take_home/cross-encoder-checkpoint'\n",
        "\n",
        "checkpoint_full_path = os.path.join(drive_path, checkpoint_name)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "metric_acc = evaluate.load(\"accuracy\")\n",
        "metric_f1 = evaluate.load(\"f1\")\n",
        "metric_precision = evaluate.load(\"precision\")\n",
        "metric_recall = evaluate.load(\"recall\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    acc = metric_acc.compute(predictions=preds, references=labels)['accuracy']\n",
        "    f1 = metric_f1.compute(predictions=preds, references=labels, average='binary')['f1']\n",
        "    precision = metric_precision.compute(predictions=preds, references=labels, average='binary')['precision']\n",
        "    recall = metric_recall.compute(predictions=preds, references=labels, average='binary')['recall']\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=checkpoint_full_path,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=2000,\n",
        "    logging_steps=1000,\n",
        "    save_steps=2000,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=2,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    fp16=True,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Find the latest checkpoint in the output directory\n",
        "latest_checkpoint = None\n",
        "\n",
        "# The 'Trainer' will look for subdirectories named 'checkpoint-XXX'\n",
        "checkpoints = [d for d in os.listdir(checkpoint_full_path) if d.startswith('checkpoint-')]\n",
        "if checkpoints:\n",
        "    # Sort to find the latest step number\n",
        "    latest_checkpoint = os.path.join(checkpoint_full_path, max(checkpoints, key=lambda x: int(x.split('-')[-1])))\n",
        "\n",
        "# If a checkpoint is found, pass its path to train()\n",
        "if latest_checkpoint:\n",
        "    print(f\"Resuming training from checkpoint: {latest_checkpoint}\")\n",
        "    trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting training from scratch.\")\n",
        "    trainer.train()\n",
        "full_path = os.path.join(drive_path, directory_name)\n",
        "\n",
        "trainer.save_model(full_path)"
      ],
      "metadata": {
        "id": "Ya29fH3kminn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reranking\n",
        "topk_standardized_product_desc\n",
        "topk_standardized_product_desc_unfiltered_with_confirmed"
      ],
      "metadata": {
        "id": "ZG9WxfZq-OIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "drive_path = '/content/drive/My Drive/'\n",
        "directory_name = 'fresho/fresho_take_home/cross-encoder-best'\n",
        "checkpoint_name = 'fresho/fresho_take_home/cross-encoder-checkpoint'\n",
        "\n",
        "checkpoint_full_path = os.path.join(drive_path, checkpoint_name)\n",
        "\n",
        "# Find the latest checkpoint in the output directory\n",
        "latest_checkpoint = None\n",
        "\n",
        "# The 'Trainer' will look for subdirectories named 'checkpoint-XXX'\n",
        "checkpoints = [d for d in os.listdir(checkpoint_full_path) if d.startswith('checkpoint-')]\n",
        "if checkpoints:\n",
        "    # Sort to find the latest step number\n",
        "    latest_checkpoint = os.path.join(checkpoint_full_path, max(checkpoints, key=lambda x: int(x.split('-')[-1])))\n",
        "\n",
        "# If a checkpoint is found, pass its path to train()\n",
        "if latest_checkpoint:\n",
        "    MODEL_DIR = latest_checkpoint\n",
        "else:\n",
        "    print(\"No checkpoint found.\")\n",
        "    MODEL_DIR = full_path\n",
        "\n",
        "# MODEL_DIR = full_path\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
        "model.eval()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "def score_pair(query_text, candidate_text):\n",
        "    inputs = tokenizer(query_text, candidate_text, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0,1]\n",
        "    return float(probs)\n",
        "\n",
        "new_suggested_product_id = []\n",
        "new_suggested_product_desc = []\n",
        "new_suggested_quantity_type_id = []\n",
        "new_scores = []\n",
        "\n",
        "for idx, row in tqdm(predictions_df.iterrows(), total=len(predictions_df)):\n",
        "    query_text = format_request(row['standardized_extracted_product_description'])\n",
        "    topk_descs = row['topk_standardized_product_desc_unfiltered_with_confirmed']\n",
        "    topk_quantity_type_ids = row['topk_qauntity_type_ids_unfiltered_with_confirmed']\n",
        "    topk_product_ids = row['topk_product_ids_unfiltered_with_confirmed']\n",
        "\n",
        "    candidate_texts = [format_candidate(n) for n in topk_descs]\n",
        "\n",
        "    scores = []\n",
        "    batch_size = 32\n",
        "    for i in range(0, len(candidate_texts), batch_size):\n",
        "        batch_cands = candidate_texts[i:i+batch_size]\n",
        "        batch_queries = [query_text]*len(batch_cands)\n",
        "        inputs = tokenizer(batch_queries, batch_cands, truncation=True, padding=True, return_tensors=\"pt\", max_length=128).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits\n",
        "            probs = torch.softmax(logits, dim=-1)[:,1].cpu().numpy()\n",
        "        scores.extend(probs.tolist())\n",
        "\n",
        "    best_idx = int(np.argmax(scores))\n",
        "    new_suggested_product_desc.append(topk_descs[best_idx])\n",
        "    new_suggested_quantity_type_id.append(topk_quantity_type_ids[best_idx])\n",
        "    new_suggested_product_id.append(topk_product_ids[best_idx])\n",
        "    new_scores.append(scores[best_idx])\n",
        "\n",
        "predictions_df['re_ranked_product_desc'] = new_suggested_product_desc\n",
        "predictions_df['re_ranked_product_id'] = new_suggested_product_id\n",
        "predictions_df['re_ranked_quantity_type_id'] = new_suggested_quantity_type_id\n",
        "predictions_df['re_ranked_score'] = new_scores\n",
        "\n",
        "drive_path = '/content/drive/My Drive/'\n",
        "directory_name = 'fresho/fresho_take_home/predictions_df.parquet'\n",
        "\n",
        "predictions_df_full_path = os.path.join(drive_path, checkpoint_name)\n",
        "\n",
        "predictions_df.to_parquet(predictions_df_full_path, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "35BwgNjTQbVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = predictions_df[['confirmed_product_id','re_ranked_product_id','confirmed_quantity_type_id','re_ranked_quantity_type_id']][(predictions_df['confirmed_quantity_type_id'] == predictions_df['re_ranked_quantity_type_id']) & (predictions_df['confirmed_product_id'] == predictions_df['re_ranked_product_id'])]\n",
        "print(f'Accuracy of Part 2 {len(results_df)/len(predictions_df)*100}')"
      ],
      "metadata": {
        "id": "Y8r1TzQYu9Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FIXME Part 3: Use XGBoost to predict quantity"
      ],
      "metadata": {
        "id": "KB8YWKLU_eZR"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "authorship_tag": "ABX9TyNJuGVd1VHUbAyWHpsyNVYk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}